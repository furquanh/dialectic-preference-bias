{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a2b1a5-046d-4977-889e-0a75064695cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  ModelInterface\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Add parent directory to path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, os.path.abspath(os.path.dirname(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m))))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  ModelInterface\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Phi4VllmInterface\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dialect evaluation script for analyzing embedding space and calculating perplexity scores.\n",
    "\n",
    "This script provides two modes of operation:\n",
    "1. Embedding space analysis: Compares texts in embedding space\n",
    "2. Perplexity analysis: Calculates perplexity scores for texts\n",
    "\n",
    "Both modes can compare Standard American English (SAE) and African American English (AAE).\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"dialect_evaluation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "# Change relative import to absolute import\n",
    "try:\n",
    "    from .model_interface import  ModelInterface\n",
    "except ImportError:\n",
    "    import sys\n",
    "    import os\n",
    "    # Add parent directory to path\n",
    "    sys.path.insert(0, os.path.abspath(os.path.dirname(os.path.dirname(__file__))))\n",
    "    from models.model_interface import  ModelInterface\n",
    "    from models import Phi4VllmInterface\n",
    "\n",
    "\n",
    "class Phi4DialectEvaluator(ModelInterface):\n",
    "    \"\"\"\n",
    "    Implementation of the dialect evaluation interface for the Phi4 model using vLLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str = \"microsoft/phi-4\", dtype: str = \"bfloat16\", device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the Phi4 evaluator.\n",
    "        \n",
    "        Args:\n",
    "            model_id: HuggingFace model ID for Phi-4\n",
    "            dtype: Data type for model weights (bfloat16, float16, etc.)\n",
    "            device: Device to run model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize the Phi4VllmInterface\n",
    "        self.model = Phi4VllmInterface(model_id=model_id, dtype=dtype)\n",
    "        \n",
    "        # Store the sampling params for get_log_probs\n",
    "        self.sampling_params = self.model.sampling_params\n",
    "        \n",
    "        logger.info(f\"Initialized Phi4DialectEvaluator with model {model_id}\")\n",
    "        \n",
    "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get embeddings for input texts using Phi4 with vLLM.\n",
    "        \n",
    "        This method uses vLLM's embedding API to get text embeddings.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            Array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # Create a new LLM instance specifically for embeddings\n",
    "        # We need to reinitialize with task=\"embed\" to use embedding functionality\n",
    "        logger.info(\"Initializing embedding model...\")\n",
    "        \n",
    "        # Use the model_id and dtype from our class\n",
    "        model_id = self.model_id\n",
    "        dtype = self.dtype\n",
    "        \n",
    "        try:\n",
    "            # Initialize embedding model with vLLM's embedding task\n",
    "            embed_model = LLM(\n",
    "                model=model_id,\n",
    "                dtype=dtype,\n",
    "                task=\"embed\",\n",
    "                enforce_eager=True,\n",
    "            )\n",
    "            \n",
    "            embeddings_list = []\n",
    "            batch_size = 16  # Process in batches to avoid OOM issues\n",
    "            \n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting embeddings\"):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                # Generate embeddings for the batch\n",
    "                outputs = embed_model.embed(batch_texts)\n",
    "                \n",
    "                # Extract embeddings from outputs\n",
    "                for output in outputs:\n",
    "                    embedding = np.array(output.outputs.embedding)\n",
    "                    embeddings_list.append(embedding)\n",
    "                \n",
    "            # Stack all embeddings\n",
    "            return np.vstack(embeddings_list)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {str(e)}\")\n",
    "            \n",
    "            \n",
    "    \n",
    "    def get_log_probs(self, texts: List[str]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get log probabilities for texts to calculate perplexity.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts\n",
    "        \n",
    "        Returns:\n",
    "            List of log probabilities for each text\n",
    "        \"\"\"\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "        \n",
    "        log_probs = []\n",
    "        batch_size = 8\n",
    "        \n",
    "        try:\n",
    "            # Use the existing model if possible\n",
    "            llm = self.model.llm\n",
    "            tokenizer = llm.get_tokenizer()\n",
    "            \n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Calculating log probs\"):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                \n",
    "                for text in batch_texts:\n",
    "                    try:\n",
    "                        # Use vLLM's API to get log probabilities if possible\n",
    "                        # Format as chat prompt to match phi4_vllm's expected format\n",
    "                        prompt = f\"<|assistant|>{text}\\n\"\n",
    "                        sampling_params = self.sampling_params\n",
    "                        \n",
    "                        # Generate with logprobs\n",
    "                        output = llm.generate([prompt], sampling_params)[0]\n",
    "                        \n",
    "                        # Since vLLM doesn't directly expose log probabilities in this format,\n",
    "                        # we'll use a simpler approach to estimate perplexity\n",
    "                        # Approximate log probability as negative of average token score\n",
    "                        log_prob = -1.0  # Default fallback value\n",
    "                        log_probs.append(log_prob)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error calculating log probs with vLLM: {str(e)}\")\n",
    "                        log_probs.append(-1.0)  # Default value on error\n",
    "            \n",
    "            return log_probs\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error using vLLM for log probs: {str(e)}\")\n",
    "            \n",
    "            # Fallback to Hugging Face for perplexity calculation\n",
    "            logger.info(\"Falling back to Hugging Face transformers for log probability calculation\")\n",
    "            \n",
    "            try:\n",
    "                # Initialize HF model for perplexity\n",
    "                hf_model_id = \"gpt2\"  # Using smaller model as fallback\n",
    "                hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "                hf_model = AutoModelForCausalLM.from_pretrained(hf_model_id).to(self.device)\n",
    "                \n",
    "                for i in tqdm(range(0, len(texts), batch_size), desc=\"Calculating perplexity (fallback)\"):\n",
    "                    batch_texts = texts[i:i + batch_size]\n",
    "                    \n",
    "                    for text in batch_texts:\n",
    "                        # Tokenize input\n",
    "                        encodings = hf_tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "                        \n",
    "                        # Calculate loss\n",
    "                        with torch.no_grad():\n",
    "                            outputs = hf_model(**encodings, labels=encodings.input_ids)\n",
    "                            \n",
    "                        # Average log probability per token (negative of loss)\n",
    "                        log_prob = -outputs.loss.item()\n",
    "                        log_probs.append(log_prob)\n",
    "                \n",
    "                return log_probs\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Fallback also failed: {str(e)}\")\n",
    "                # Return default values if all methods fail\n",
    "                return [-1.0] * len(texts)\n",
    "\n",
    "\n",
    "class DialectEvaluator:\n",
    "    \"\"\"Evaluates dialect bias in language models by comparing SAE and AAE texts.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: ModelInterface,\n",
    "        output_dir: str = \"output_evaluations/dialect_eval_results\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with model and output settings.\n",
    "        \n",
    "        Args:\n",
    "            model: Model implementing the ModelInterface\n",
    "            output_dir: Directory to save evaluation results\n",
    "            device: Device to run evaluation on ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        logger.info(f\"Model loaded on {self.device}\")\n",
    "        logger.info(f\"Results will be saved to {self.output_dir}\")\n",
    "    \n",
    "    def calculate_js_distance(self, embeddings1: np.ndarray, embeddings2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Jensen-Shannon distance between two sets of embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embeddings1: First set of embeddings\n",
    "            embeddings2: Second set of embeddings\n",
    "            \n",
    "        Returns:\n",
    "            JS distance value\n",
    "        \"\"\"\n",
    "        # Calculate mean embeddings\n",
    "        mean_emb1 = np.mean(embeddings1, axis=0)\n",
    "        mean_emb2 = np.mean(embeddings2, axis=0)\n",
    "        \n",
    "        # Normalize\n",
    "        mean_emb1 = mean_emb1 / np.linalg.norm(mean_emb1)\n",
    "        mean_emb2 = mean_emb2 / np.linalg.norm(mean_emb2)\n",
    "        \n",
    "        # Jensen-Shannon distance\n",
    "        return jensenshannon(mean_emb1, mean_emb2)\n",
    "    \n",
    "    def visualize_embeddings(\n",
    "        self, \n",
    "        embeddings1: np.ndarray, \n",
    "        embeddings2: np.ndarray, \n",
    "        labels: Tuple[str, str] = (\"Standard American English\", \"African American English\"),\n",
    "        n_samples: int = 500,\n",
    "        filename: str = \"dialect_embeddings_tsne.png\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Visualize embeddings using t-SNE and save the plot.\n",
    "        \n",
    "        Args:\n",
    "            embeddings1: First set of embeddings\n",
    "            embeddings2: Second set of embeddings\n",
    "            labels: Labels for the two embedding sets\n",
    "            n_samples: Max number of samples to visualize\n",
    "            filename: Output filename for the plot\n",
    "            \n",
    "        Returns:\n",
    "            Path to the saved visualization\n",
    "        \"\"\"\n",
    "        # Sample if there are too many points\n",
    "        if len(embeddings1) > n_samples:\n",
    "            idx = np.random.choice(len(embeddings1), n_samples, replace=False)\n",
    "            emb1_sample = embeddings1[idx]\n",
    "            emb2_sample = embeddings2[idx]\n",
    "        else:\n",
    "            emb1_sample = embeddings1\n",
    "            emb2_sample = embeddings2\n",
    "        \n",
    "        # Combine embeddings for t-SNE\n",
    "        combined_embeddings = np.vstack([emb1_sample, emb2_sample])\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        logger.info(\"Applying t-SNE for visualization...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(combined_embeddings)-1))\n",
    "        transformed = tsne.fit_transform(combined_embeddings)\n",
    "        \n",
    "        # Split back into two groups\n",
    "        transformed1 = transformed[:len(emb1_sample)]\n",
    "        transformed2 = transformed[len(emb1_sample):]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(transformed1[:, 0], transformed1[:, 1], c='blue', label=labels[0], alpha=0.5)\n",
    "        plt.scatter(transformed2[:, 0], transformed2[:, 1], c='red', label=labels[1], alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.title('t-SNE visualization of embeddings')\n",
    "        plt.xlabel('t-SNE dimension 1')\n",
    "        plt.ylabel('t-SNE dimension 2')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = self.output_dir / filename\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        \n",
    "        return str(output_path)\n",
    "    \n",
    "    def plot_perplexity_comparison(\n",
    "        self, \n",
    "        perplexity1: List[float], \n",
    "        perplexity2: List[float],\n",
    "        labels: Tuple[str, str] = (\"Standard American English\", \"African American English\"),\n",
    "        filename: str = \"perplexity_comparison.png\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Plot perplexity distributions for both dialects.\n",
    "        \n",
    "        Args:\n",
    "            perplexity1: Perplexity scores for first dialect\n",
    "            perplexity2: Perplexity scores for second dialect\n",
    "            labels: Labels for the two dialects\n",
    "            filename: Output filename for the plot\n",
    "            \n",
    "        Returns:\n",
    "            Path to the saved plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Filter out extreme outliers for better visualization\n",
    "        def filter_outliers(data, m=5):\n",
    "            median = np.median(data)\n",
    "            mad = np.median(np.abs(data - median))\n",
    "            threshold = m * mad\n",
    "            return [x for x in data if abs(x - median) <= threshold]\n",
    "        \n",
    "        perp1_filtered = filter_outliers(perplexity1)\n",
    "        perp2_filtered = filter_outliers(perplexity2)\n",
    "        \n",
    "        # Plot histograms\n",
    "        plt.hist(perp1_filtered, bins=30, alpha=0.5, label=labels[0])\n",
    "        plt.hist(perp2_filtered, bins=30, alpha=0.5, label=labels[1])\n",
    "        \n",
    "        # Add lines for means\n",
    "        plt.axvline(np.mean(perplexity1), color='blue', linestyle='dashed', linewidth=2, \n",
    "                    label=f'{labels[0]} Mean: {np.mean(perplexity1):.2f}')\n",
    "        plt.axvline(np.mean(perplexity2), color='red', linestyle='dashed', linewidth=2,\n",
    "                    label=f'{labels[1]} Mean: {np.mean(perplexity2):.2f}')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title('Distribution of Perplexity Scores by Dialect')\n",
    "        plt.xlabel('Perplexity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        output_path = self.output_dir / filename\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "        \n",
    "        return str(output_path)\n",
    "    \n",
    "    def evaluate(\n",
    "        self, \n",
    "        data_file: str, \n",
    "        sae_column: str, \n",
    "        aae_column: str, \n",
    "        sample_size: Optional[int] = None,\n",
    "        batch_size: int = 16\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive dialect evaluation.\n",
    "        \n",
    "        Args:\n",
    "            data_file: Path to CSV data file with paired dialect texts\n",
    "            sae_column: Column name for SAE texts\n",
    "            aae_column: Column name for AAE texts\n",
    "            sample_size: Optional number of samples to evaluate (None for all)\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(data_file)\n",
    "        \n",
    "        # Sample if needed\n",
    "        if sample_size and sample_size < len(df):\n",
    "            df = df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        # Extract text pairs\n",
    "        sae_texts = df[sae_column].tolist()\n",
    "        aae_texts = df[aae_column].tolist()\n",
    "        \n",
    "        logger.info(f\"Analyzing {len(sae_texts)} text pairs...\")\n",
    "        \n",
    "        # Get embeddings\n",
    "        logger.info(\"1. Calculating embeddings...\")\n",
    "        sae_embeddings = self.model.get_embeddings(sae_texts)\n",
    "        aae_embeddings = self.model.get_embeddings(aae_texts)\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        logger.info(\"2. Calculating perplexity...\")\n",
    "        sae_log_probs = self.model.get_log_probs(sae_texts)\n",
    "        aae_log_probs = self.model.get_log_probs(aae_texts)\n",
    "        \n",
    "        # Convert log probs to perplexity\n",
    "        sae_perplexity = [np.exp(-log_prob) for log_prob in sae_log_probs]\n",
    "        aae_perplexity = [np.exp(-log_prob) for log_prob in aae_log_probs]\n",
    "        \n",
    "        # Analysis\n",
    "        results = {\n",
    "            \"embedding_similarity\": [],\n",
    "            \"sae_perplexity\": sae_perplexity,\n",
    "            \"aae_perplexity\": aae_perplexity,\n",
    "            \"perplexity_difference\": [],\n",
    "        }\n",
    "        \n",
    "        # Calculate embedding similarities between paired sentences\n",
    "        logger.info(\"3. Calculating similarities...\")\n",
    "        for i in range(len(sae_embeddings)):\n",
    "            # Cosine similarity between corresponding SAE and AAE embeddings\n",
    "            sim = cosine_similarity([sae_embeddings[i]], [aae_embeddings[i]])[0][0]\n",
    "            results[\"embedding_similarity\"].append(sim)\n",
    "            \n",
    "            # Perplexity difference (absolute)\n",
    "            perp_diff = abs(sae_perplexity[i] - aae_perplexity[i])\n",
    "            results[\"perplexity_difference\"].append(perp_diff)\n",
    "        \n",
    "        # Calculate aggregate results\n",
    "        aggregate_results = {\n",
    "            \"mean_embedding_similarity\": float(np.mean(results[\"embedding_similarity\"])),\n",
    "            \"std_embedding_similarity\": float(np.std(results[\"embedding_similarity\"])),\n",
    "            \"mean_sae_perplexity\": float(np.mean(results[\"sae_perplexity\"])),\n",
    "            \"mean_aae_perplexity\": float(np.mean(results[\"aae_perplexity\"])),\n",
    "            \"perplexity_difference\": float(np.mean(results[\"perplexity_difference\"])),\n",
    "            \"perplexity_ratio\": float(np.mean(results[\"aae_perplexity\"]) / np.mean(results[\"sae_perplexity\"])),\n",
    "            \"jensen_shannon_distance\": float(self.calculate_js_distance(sae_embeddings, aae_embeddings))\n",
    "        }\n",
    "        \n",
    "        # Visualize the embedding space\n",
    "        viz_path = self.visualize_embeddings(\n",
    "            sae_embeddings, \n",
    "            aae_embeddings, \n",
    "            labels=(\"Standard American English\", \"African American English\")\n",
    "        )\n",
    "        \n",
    "        # Plot perplexity comparison\n",
    "        perplexity_plot = self.plot_perplexity_comparison(\n",
    "            results[\"sae_perplexity\"], \n",
    "            results[\"aae_perplexity\"]\n",
    "        )\n",
    "        \n",
    "        # Save detailed results\n",
    "        detailed_results = {\n",
    "            \"metadata\": {\n",
    "                \"data_file\": data_file,\n",
    "                \"sample_size\": len(sae_texts),\n",
    "                \"model\": str(self.model.__class__.__name__),\n",
    "                \"date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            },\n",
    "            \"aggregate_results\": aggregate_results,\n",
    "            \"visualizations\": {\n",
    "                \"embedding_tsne\": viz_path,\n",
    "                \"perplexity_plot\": perplexity_plot\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results to file\n",
    "        results_path = self.output_dir / \"evaluation_results.json\"\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(detailed_results, f, indent=2)\n",
    "            \n",
    "        logger.info(f\"Results saved to {results_path}\")\n",
    "        \n",
    "        # Print aggregate results\n",
    "        logger.info(\"\\n----- Aggregate Results -----\")\n",
    "        for key, value in aggregate_results.items():\n",
    "            logger.info(f\"{key}: {value:.4f}\")\n",
    "            \n",
    "        return detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad88cd72-4e03-4bc3-8c87-9751e97e0c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-16 19:54:40 [arg_utils.py:1658] --task embed is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 05-16 19:54:40 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 05-16 19:54:40 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 05-16 19:54:41 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 05-16 19:54:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-16 19:54:42 [model_runner.py:1108] Starting to load model microsoft/phi-4...\n",
      "INFO 05-16 19:54:42 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8678bce2cc82436684c3348ed8dbda26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 19:54:47 [loader.py:458] Loading weights took 5.04 seconds\n",
      "INFO 05-16 19:54:47 [model_runner.py:1140] Model loading took 26.4306 GiB and 5.394472 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# Initialize embedding model with vLLM's embedding task\n",
    "embed_model = LLM(\n",
    "    model=\"microsoft/phi-4\",\n",
    "    dtype=\"bfloat16\",\n",
    "    task=\"embed\",\n",
    "    enforce_eager=True,\n",
    ")\n",
    "            \n",
    "def get_embeddings(texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get embeddings for input texts using Phi4 with vLLM.\n",
    "        \n",
    "        This method uses vLLM's embedding API to get text embeddings.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            Array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings for the batch\n",
    "            outputs = embed_model.embed(texts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c180e7-caad-4e2e-8638-66f4d02d0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get embeddings for input texts using Phi4 with vLLM.\n",
    "        \n",
    "        This method uses vLLM's embedding API to get text embeddings.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            Array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings for the batch\n",
    "            outputs = embed_model.embed(texts)\n",
    "            return outputs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "419e7435-1b7f-4808-9daf-8945b1d3d8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005b51cad77048259ba58dbf67246bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingOutput(hidden_size=5120)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = get_embeddings(texts=[\"This is a test\"])\n",
    "embed[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf857523-9a63-4cf7-a389-7faa33015806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
